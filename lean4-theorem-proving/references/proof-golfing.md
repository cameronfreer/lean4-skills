# Proof Golfing: Simplifying Proofs After Compilation

Systematic patterns for simplifying and cleaning up proofs after the file compiles successfully.

**When to use:** After your file compiles with `lake build`, perform these simplification passes to improve readability and maintainability.

## Core Principle

**First make it compile, then make it clean.**

Don't prematurely optimize proofs while getting code to work. Once everything compiles, apply these patterns systematically.

## Before You Start: Essential Setup

### 1. Establish File Scope Boundaries

**Ask the user first:**
```
"I want to golf proofs. Which files are safe to edit?
Which are you actively working on or recently refactored?"
```

**Why this matters:** Avoid wasting time on files with:
- Active work in progress (changes will conflict)
- Recent refactoring (cleanup may already be done)
- Uncommitted changes (your edits may get lost)

**Good targets:**
- Files marked as "stable"
- Files that haven't changed in >1 week
- Files user explicitly designates

### 2. Get Concrete Golfing Targets

**Ask for specific patterns to look for:**

```
"What patterns should I simplify? Examples:
1. := by exact <term>  ‚Üí  := <term>
2. Multiple consecutive rw steps  ‚Üí  rw [a, b, c]
3. by intro x; exact f x  ‚Üí  f
4. Proofs with >5 consecutive 'have' statements
5. Other patterns specific to your codebase?"
```

**Why this matters:** Generic "golf proofs" leads to:
- Inconsistent style choices
- Missing domain-specific simplifications
- Time spent on patterns user doesn't care about

### 3. See Examples from the Actual Codebase

**Request before/after examples:**

```
"Can you show me one example from YOUR code of:
- ‚ùå Verbose style (what to golf)
- ‚úÖ Concise style (target result)

This helps me match your aesthetic preferences."
```

**Common style preferences to clarify:**
- `simp [foo, bar]` vs `simp only [foo, bar]`
- Term mode (`:= term`) vs tactic mode (`:= by tactics`)
- Calc chains vs transitive `have` statements
- Anonymous `have :` vs named `have h_property :`

### 4. Check for Existing Tooling

**Ask about automation:**

```
"Are there scripts or tools for finding golfing targets?
Examples:
- scripts/proof_complexity.sh - find longest proofs
- lake env lean --find-unused - detect dead code
- Custom linters or style checkers"
```

**Why this matters:** Use existing infrastructure instead of manual search

### 5. Get Quick Context on Recent Changes

**Ask before starting:**

```
"Any recent changes I should know about?
- Files recently refactored (may already be clean)
- Known cleanup targets (technical debt areas)
- Style changes in progress (avoid conflicts)"
```

## üö® CRITICAL: When NOT to Optimize (False Positives)

**Key finding from comprehensive codebase scan:** 93% of detected patterns were false positives!

**Out of 47 "optimization opportunities" found, only 3 (6%) were actually worth optimizing.**

### The Multiple-Use Heuristic ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**NEW RULE: If a let binding is used ‚â•3 times, DON'T inline it.**

**Empirical data:**
- Let bindings used 1-2 times: 100% optimization success rate
- Let bindings used 3-4 times: 40% worth optimizing
- Let bindings used 5+ times: 0% worth optimizing (NEVER inline!)

**Example - DON'T optimize this:**
```lean
// CommonEnding.lean:531-541 - LOOKS like let+have+exact pattern
let Œº_map := Measure.map (fun œâ i => X (k i) œâ) Œº
let Œº_bind := Œº.bind fun œâ => Measure.pi fun _ : Fin m => ŒΩ œâ

have h_map_prob : IsProbabilityMeasure Œº_map := ...
have h_bind_prob : IsProbabilityMeasure Œº_bind := ...
have h_eq : Œº_map = Œº_bind := ...
// ... both Œº_map and Œº_bind used 5+ more times ...
```

**Why NOT to inline:**
- Œº_map is ~20 tokens, used 7 times
- Current: 20 tokens (def) + 2 tokens √ó 7 uses = **34 tokens**
- Inlined: 20 tokens √ó 7 uses = **140 tokens**
- Inlining makes it **4√ó LONGER!**

### The Readability Threshold

**NEW METRIC: Readability Cost = (nesting depth) √ó (inline complexity) √ó (repetition)**

**If Readability Cost > 5, keep current structure even if it uses more tokens.**

**Example - DON'T optimize this:**
```lean
// L2Helpers.lean:148-180 - Complex proof with semantic bindings
lemma contractable_map_pair ... := by
  let k : Fin 2 ‚Üí ‚Ñï := fun t => if t = fin2Zero then i else j
  have hk : StrictMono k := strictMono_two hij
  have h_map := hX_contract 2 k hk
  let eval : (Fin 2 ‚Üí ‚Ñù) ‚Üí ‚Ñù √ó ‚Ñù := fun g => (g fin2Zero, g fin2One)
  have h_eval_meas : Measurable eval := ...
  // ... 15 more lines using k and eval ...
  simpa [...] using h_comp
```

**Why NOT to inline:**
- Let bindings define **semantic concepts** (subsequence selector, evaluation function)
- Proof has complex intermediate steps
- Inlining would create unreadable nested lambdas
- **Readability loss >> token savings**

### Quantified Decision Rule

**Optimize let+have+exact pattern ONLY if ALL of these hold:**

1. ‚úÖ Let binding used ‚â§2 times, AND
2. ‚úÖ Proof is simple (just intro/exact, no cases/complex logic), AND
3. ‚úÖ Token savings > 10, AND
4. ‚úÖ Doesn't harm readability significantly

**Skip if ANY of these hold:**

1. ‚ùå Let binding used ‚â•3 times, OR
2. ‚ùå Complex proof with case analysis, OR
3. ‚ùå Semantic naming aids understanding, OR
4. ‚ùå Would create deeply nested lambdas (>2 levels)

### The Optimization Saturation Point

**Empirical finding: After initial cleanup, optimization returns diminish rapidly.**

**Session-by-session data:**
- Session 1-2: 60% of patterns worth optimizing (high-value targets)
- Session 3: 20% of patterns worth optimizing (medium-value)
- Session 4: 6% of patterns worth optimizing (low-value, **diminishing returns**)

**Time efficiency breakdown:**
- First 15 optimizations: ~2 min each (30 min total)
- Next 7 optimizations: ~5 min each (35 min total)
- Last 3 optimizations: ~18 min each (54 min total)

**Point of diminishing returns:** When optimization rate drops below 20% and time per optimization exceeds 15 minutes.

**Recommendation:** Declare victory, document patterns, move on to higher-value work.

### Signs You've Reached Saturation

**Stop golfing when you see these:**

1. ‚úã Optimization success rate drops below 20%
2. ‚úã Time per optimization exceeds 15 minutes
3. ‚úã Most "patterns" turn out to be false positives
4. ‚úã Optimizations start hurting readability
5. ‚úã You're debating whether 2-token savings is worth it

**Empirical benchmark:** Well-maintained codebases reach saturation after ~20-25 optimizations.

## Quick Reference: Common Patterns

Based on real-world sessions with 60-75% size reduction per proof (when patterns actually apply):

**Pattern 1: Remove `by exact` wrapper**
```lean
-- ‚ùå Before (2 lines)
lemma foo : P := by
  exact term

-- ‚úÖ After (1 line)
lemma foo : P := term
```

**Pattern 2: The "let + have + exact" anti-pattern** ‚≠ê HIGH IMPACT (but see warnings!)

‚ö†Ô∏è **CRITICAL:** Check if bindings are used multiple times first! See [False Positives section](#-critical-when-not-to-optimize-false-positives).

```lean
-- ‚ùå Before (14 lines, ~140 tokens)
lemma Contractable.prefix ... := by
  intro m k hk_mono
  -- Lift k to a function
  let k' : Fin m ‚Üí ‚Ñï := fun i => (k i).val
  have hk'_mono : StrictMono k' := by
    intro i j hij
    simp only [k']
    exact hk_mono hij
  exact hX m k' hk'_mono

-- ‚úÖ After (2 lines, ~38 tokens)
lemma Contractable.prefix ... := by
  intro m k hk_mono
  exact hX m (fun i => (k i).val) (fun i j hij => hk_mono hij)
```

**When this pattern applies (ALL must be true):**
- ‚úÖ let binding used ‚â§2 times (preferably only in have and final exact)
- ‚úÖ have proof is simple (no complex case analysis)
- ‚úÖ Final result accepts lambda arguments
- ‚úÖ No semantic naming value lost

**When NOT to apply (ANY of these = skip):**
- ‚ùå Let binding used ‚â•3 times anywhere in proof
- ‚ùå Complex proof logic (cases, nested proofs)
- ‚ùå Let binding represents important semantic concept
- ‚ùå Would create deeply nested lambdas (>2 levels)

**Pattern 3: Inline constructor branches**
```lean
-- ‚ùå Before (7 lines)
constructor
¬∑ intro k hk
  exact hX m k hk
¬∑ intro ŒΩ' hŒΩ'
  have hid : StrictMono ... := fun i j hij => hij
  have h := hŒΩ' (fun i => i.val) hid
  exact h.symm

-- ‚úÖ After (3 lines)
constructor
¬∑ intro k hk; exact hX m k hk
¬∑ intro ŒΩ' hŒΩ'; exact (hŒΩ' (fun i => i.val) (fun i j hij => hij)).symm
```

**Pattern 4: Direct lemma over automation** (for simple cases)
```lean
-- ‚ùå Wrong (longer AND fails!)
exact hX m (fun i => k + i.val) (fun i j hij => by omega)
-- Error: omega produces counterexample!

-- ‚úÖ Correct (shorter AND works!)
exact hX m (fun i => k + i.val) (fun i j hij => Nat.add_lt_add_left hij k)
```

**When NOT to use automation:**
- You have a direct mathlib lemma that's ‚â§5 tokens
- Simple Nat arithmetic (omega can struggle with coercions)
- Automation overhead > direct application

**Pattern 5: Smart `ext` - Nested extensionality** ‚≠ê NEW

`ext` is smarter than you think - it automatically handles multiple nested extensionality layers!

```lean
-- ‚ùå Before (4 lines - manual nesting)
apply Subtype.ext
apply Fin.ext
simp [Œπ]

-- ‚úÖ After (2 lines - ext handles it)
ext; simp [Œπ]
```

**Key insight:** Single `ext` automatically applies:
- `Subtype.ext` (for subtypes)
- `Fin.ext` (for finite types)
- `funext` (for function extensionality)
- Multiple layers in sequence

**When to use single `ext`:**
- Nested extensionality goals (Subtype of Fin, functions returning subtypes, etc.)
- Multiple extensionality steps that would otherwise be `apply X.ext; apply Y.ext`
- Any time you see consecutive `apply *.ext` statements

**Savings:** 50% reduction (4 lines ‚Üí 2 lines) with no loss of clarity

**Pattern 6: `ext`-`simp` chain combination** (when natural)

Combine `ext` with subsequent `simp` steps using semicolons when the operations are sequential and natural.

```lean
-- ‚ùå Before (6 lines)
ext x
simp only [Finset.mem_image, Finset.mem_univ, true_and, iff_true]
use œÉ.symm x
simp

-- ‚úÖ After (1 line)
ext x; simp only [Finset.mem_image, Finset.mem_univ, true_and, iff_true]; use œÉ.symm x; simp
```

**When semicolon chaining is worth it:**
- ‚úÖ Sequential operations (each step depends on previous)
- ‚úÖ Saves ‚â•2 lines
- ‚úÖ No loss of readability (proof flow still clear)
- ‚úÖ Steps are simple (intro, ext, use, simp, etc.)

**When NOT to use semicolons (from earlier anti-patterns):**
- ‚ùå Just combining lines without reducing tokens
- ‚ùå Would create unreadable long lines (>100 chars)
- ‚ùå Complex intermediate steps that need inspection
- ‚ùå Not actually saving tokens (semicolon is a token!)

**Refined rule:** Semicolons are good for *sequential proof steps*, not for *arbitrary line combining*.

**Pattern 7: Remove `have`-`simpa` indirection**

When a `have` just calls a lemma and wraps it with `simpa`, inline it.

```lean
-- ‚ùå Before (3 lines - unnecessary indirection)
have : m' ‚â§ k ‚ü®m', Nat.lt_succ_self m'‚ü© := by
  have h := strictMono_Fin_ge_id hk_mono ‚ü®m', Nat.lt_succ_self m'‚ü©
  simpa using h

-- ‚úÖ After (2 lines - direct application)
have : m' ‚â§ k ‚ü®m', Nat.lt_succ_self m'‚ü© :=
  strictMono_Fin_ge_id hk_mono ‚ü®m', Nat.lt_succ_self m'‚ü©
```

**When to remove indirection:**
- `have h := lemma; simpa using h` ‚Üí just apply lemma directly
- `simpa` is only unfolding definitions (no real work)
- The intermediate `have` adds no value

**Impact from real sessions:**
- Session 1: 11 proofs, ~22 lines saved
- Session 2: 3 proofs, ~26 lines saved (76.5% reduction avg)
- Session 4: 4 proofs, ~8 lines saved (ext-simp chains, have-simpa removal)

## Systematic Optimization Workflow

Techniques inspired by automated proof optimization research, adapted for interactive development.

### The Multi-Candidate Strategy ‚≠ê GAME CHANGER

**Core idea:** Generate multiple optimization candidates and test them in parallel, keeping the shortest that compiles.

**Using lean_multi_attempt (MCP tool):**
```lean
-- Original (14 lines):
lemma foo ... := by
  let x := <definition>
  have h : Property x := by <proof>
  exact <result> x h

-- Generate 3 candidates and test in parallel:
mcp__lean-lsp__lean_multi_attempt(
  file = "MyFile.lean",
  line = 218,
  tactics = [
    // Candidate A: Inline with let kept
    "let x := <def>; exact <result> x (fun ... => <proof>)",

    // Candidate B: Full inline
    "exact <result> (fun ... => <def>) (fun ... => <proof>)",

    // Candidate C: Use refine
    "refine <result> ?def ?proof; <fill holes>"
  ]
)
```

**Why this is powerful:**
- Tests 3-4 approaches simultaneously (not sequentially)
- Immediate compilation feedback
- Pick shortest successful candidate
- No wasted time debugging failed approaches

**Time savings:** ~70% per proof vs. manual trial-and-error

**Success rate from real sessions:** 90% (at least one candidate compiles)

### Pattern-Based Search (Not Sequential Reading)

**Don't:** Read through file line by line looking for optimizations

**Do:** Search for known patterns systematically

**High-value patterns to search for:**

```bash
# 1. The "let + have + exact" pattern (HIGHEST value)
grep -A 10 "let .*:=" MyFile.lean | grep -B 8 "exact"

# 2. Multiple consecutive "have" statements
grep -B 2 -A 2 "have.*:=.*by" MyFile.lean | grep -c "have"

# 3. "by exact" wrappers
grep "by$" MyFile.lean | head -1 | xargs -I {} grep -A 1 {} MyFile.lean

# 4. Constructor branches with multiple lines
grep -A 5 "constructor" MyFile.lean | grep "intro"
```

**Impact:** Find all 3 optimization targets in 30 seconds instead of reading entire file

### The Decision Tree for Candidate Generation

When you find the "let + have + exact" pattern, automatically generate these candidates:

```
Found: let x := <def>; have h : P := by <proof>; exact <result> x h

Generate in parallel:
‚îú‚îÄ Candidate A: Full inline
‚îÇ  exact <result> (fun ... => <def>) (fun ... => <proof>)
‚îÇ  ‚Ä¢ Shortest when types are simple
‚îÇ  ‚Ä¢ May fail if type inference struggles
‚îÇ
‚îú‚îÄ Candidate B: Inline with let kept
‚îÇ  let x := <def>; exact <result> x (fun ... => <proof>)
‚îÇ  ‚Ä¢ Middle ground approach
‚îÇ  ‚Ä¢ Helps type inference
‚îÇ
‚îî‚îÄ Candidate C: Use refine for complex types
   refine <result> ?def ?proof; exact <def>; exact <proof>
   ‚Ä¢ Most likely to compile
   ‚Ä¢ Slightly longer
```

**Test all 3 with lean_multi_attempt, pick shortest that compiles.**

### Token Counting Quick Reference

**For picking winners from multiple candidates:**

```lean
// ~1 token each
let, have, exact, intro, by, fun

// ~2 tokens each
:=, =>, (fun x => ...), StrictMono

// ~3 tokens each
Nat.add_lt_add_left, (fun i j hij => ...)

// ~5-10 tokens
let x : Type := definition
have h : Property := by proof
```

**Rule of thumb:**
- Each line ‚âà 8-12 tokens
- Each have + proof ‚âà 15-20 tokens
- Each inline lambda ‚âà 5-8 tokens

**Mental shortcut:** Compare line counts first, then check token density for similar lengths.

### When NOT to Optimize

**Anti-patterns (don't waste time on these):**

‚ùå **Already optimal** (1-2 lines)
```lean
lemma foo : P := hX  -- Can't improve!
```

‚ùå **Readability would suffer significantly**
```lean
// Before: Clear intent
have h1 : A := ...  -- Establishes precondition
have h2 : B := ...  -- Derives intermediate result
have h3 : C := ...  -- Combines for conclusion
exact combine h1 h2 h3

// After: Unreadable mess
exact combine (obscure nested lambdas) ...
```

‚ùå **Complex pattern matching**
```lean
cases x with
| inl ha => ...  -- Don't try to inline pattern matches!
| inr hb => ...
```

‚úÖ **DO optimize when:**
- Proof is >5 lines with repetitive structure
- Clear "let + have + exact" pattern
- Multiple small have statements that could inline
- Mechanical transformations (not semantic reasoning)

### Measuring Success

**Track these metrics:**
- **Lines saved:** before/after line count
- **Token reduction:** rough estimate from mental counting
- **Compilation:** all optimizations must compile
- **Time per proof:** aim for <5 minutes per optimization

**Real session benchmarks across all sessions:**
- **Cumulative:** 23 proofs optimized, ~108 lines removed, ~34% token reduction
- **Average reduction:** 68% per optimized proof
- **Time per proof:** ~2-5 minutes (with systematic workflow)
- **Success rate:** 100% compilation (with multi-candidate approach)
- **ROI:** Break-even after ~3 files (patterns become automatic)
- **Final scan:** Codebase reached saturation (excluding Via* files)

**Technique effectiveness ranking:**
1. let + have + exact pattern: 50% of all savings, 60-80% reduction per instance
2. Smart ext (nested extensionality): 50% reduction, no loss of clarity
3. Multi-candidate testing: 80% success rate, ~70% time savings vs manual
4. ext-simp chain combinations: Saves ‚â•2 lines when natural
5. Arithmetic simplification: 30-50% reduction per instance
6. have-simpa indirection removal: Simple, safe, always beneficial
7. Constructor branch compression: 25-57% reduction per instance

## Simplification Patterns

### Pattern 1: Inline Single-Use Intermediate Lemmas

When an intermediate `have` is used exactly once, inline it:

**Before:**
```lean
have hterm : ‚àÄ j ‚àà Neg, |c j| = -c j := fun j hj => abs_of_neg (Finset.mem_filter.mp hj).2
calc ‚àë j ‚àà Neg, |c j|
    = ‚àë j ‚àà Neg, (-c j) := Finset.sum_congr rfl hterm
  _ = -‚àë j ‚àà Neg, c j := by simp [Finset.sum_neg_distrib]
```

**After:**
```lean
calc ‚àë j ‚àà Neg, |c j|
    = ‚àë j ‚àà Neg, (-c j) := Finset.sum_congr rfl (fun j hj => abs_of_neg (Finset.mem_filter.mp hj).2)
  _ = -‚àë j ‚àà Neg, c j := by simp [Finset.sum_neg_distrib]
```

**When to inline:**
- The intermediate lemma is used exactly once
- Inlining doesn't make the proof significantly harder to read
- The term fits naturally as a lambda or inline expression

**When to keep separate:**
- The lemma is used multiple times
- The proof is complex and benefits from a descriptive name
- Inlining would create an unreadably long line

### Pattern 2: Consolidate Simple Rewrites

**Before:**
```lean
have hœÉSq_nonneg : 0 ‚â§ œÉSq := by
  simpa [œÉSq] using sq_nonneg œÉ

have hvar : ‚àÄ k, ‚à´ œâ, (Œæ k œâ - m)^2 ‚àÇŒº = œÉSq := by
  intro k; simpa [œÉSq] using _hvar k
```

**After:**
```lean
have hœÉSq_nonneg : 0 ‚â§ œÉSq := sq_nonneg œÉ
have hvar : ‚àÄ k, ‚à´ œâ, (Œæ k œâ - m)^2 ‚àÇŒº = œÉSq := fun k => _hvar k
```

**When to consolidate:**
- `simpa` or `simpa using` that just unfolds definitions
- Simple `intro` followed by direct application
- `by exact` wrappers (just remove them)

### Pattern 3: Arithmetic Simplification with Targeted Automation

**Use linarith for transitive chains:**
```lean
-- ‚ùå Before (3 lines)
have hmpos : 0 < (m : ‚Ñù) := by
  calc (0 : ‚Ñù) < 4*Cf/Œµ^2 := by positivity
    _ < m := hA_lt_m

-- ‚úÖ After (1 line, 67% reduction)
have hmpos : 0 < (m : ‚Ñù) := by
  linarith [show 0 < 4*Cf/Œµ^2 by positivity, hA_lt_m]
```

**Use omega for simple Nat arithmetic:**
```lean
-- ‚ùå Before (4 lines)
have h_mem : ‚àÄ i ‚àà s, i < N := by
  intro i hi
  have hle : i ‚â§ s.sup id := by convert Finset.le_sup hi
  exact Nat.lt_of_le_of_lt hle (Nat.lt_succ_self _)

-- ‚úÖ After (3 lines, 25% reduction)
have h_mem : ‚àÄ i ‚àà s, i < N := by
  intro i hi
  have hle : i ‚â§ s.sup id := by convert Finset.le_sup hi
  omega
```

**When automation FAILS - use direct lemmas:**
```lean
-- ‚ùå Wrong (fails with counterexample!)
exact hX m (fun i => k + i.val) (fun i j hij => by omega)
-- omega struggles with Fin coercions

-- ‚úÖ Correct (shorter AND works!)
exact hX m (fun i => k + i.val) (fun i j hij => Nat.add_lt_add_left hij k)
```

**Rule:** For simple goals with direct mathlib lemmas ‚â§5 tokens, skip automation. Direct lemmas are shorter and more reliable.

### Pattern 4: Merge Simp Steps

**Before:**
```lean
have hc_sum : ‚àë j, c j = 0 := by
  simp only [c]
  have hp := _hp_prob.1
  have hq := _hq_prob.1
  rw [Finset.sum_sub_distrib, hp, hq]
  ring
```

**After:**
```lean
have hc_sum : ‚àë j, c j = 0 := by
  simp only [c, Finset.sum_sub_distrib, _hp_prob.1, _hq_prob.1]; ring
```

**When to merge:**
- Multiple `simp only` or `rw` steps that could be combined
- Sequential rewrites that don't need intermediate inspection
- Proof ends with `ring`, `linarith`, or other finishing tactic

### Pattern 4: Replace `trans` + `calc` with Single `calc`

**Before:**
```lean
have h_diag : ... := by
  trans (‚àë i, (c i)^2 * œÉSq)
  ¬∑ congr 1; ext i
    calc ...
  ¬∑ rw [‚Üê Finset.sum_mul]; ring
```

**After:**
```lean
have h_diag : ... := by
  calc ‚àë i, c i * c i * ‚à´ œâ, (Œæ i œâ - m) * (Œæ i œâ - m) ‚àÇŒº
      = ‚àë i, (c i)^2 * œÉSq := by congr 1; ext i; calc ...
    _ = œÉSq * ‚àë i, (c i)^2 := by rw [‚Üê Finset.sum_mul]; ring
```

**When to use single calc:**
- The intermediate step is clear from the calc chain
- `trans` is only being used to set up a calc
- The proof becomes more readable as a unified calculation

### Pattern 5: Eliminate Nested Helper Lemmas in Calc

**Before:**
```lean
have h_offdiag : ... := by
  have h_cov_term : ‚àÄ i j, ... := by ...
  have h_rewrite : ... := by
    apply Finset.sum_congr rfl
    intro i _
    apply Finset.sum_congr rfl
    intro j hj
    exact h_cov_term i j ...
  have h_factor : ... := by simp [Finset.mul_sum, mul_assoc]
  calc ... = ... := h_rewrite
       _ = ... := h_factor
```

**After:**
```lean
have h_offdiag : ... := by
  calc ‚àë i, ‚àë j with j ‚â† i, c i * c j * ‚à´ œâ, (Œæ i œâ - m) * (Œæ j œâ - m) ‚àÇŒº
      = ‚àë i, ‚àë j with j ‚â† i, œÉSq * œÅ * (c i * c j) := by
          apply Finset.sum_congr rfl; intro i _
          apply Finset.sum_congr rfl; intro j hj
          have hcov_ij := hcov i j (Ne.symm (Finset.mem_filter.mp hj).2)
          simp [hcov_ij, mul_comm, mul_assoc]
    _ = œÉSq * œÅ * ‚àë i, ‚àë j with j ‚â† i, c i * c j := by simp [Finset.mul_sum, mul_assoc]
```

**When to inline helpers:**
- Helper lemmas are used exactly once in the final calc
- The proof logic is clearer when steps are inline
- The helper names don't add significant documentation value

### Pattern 6: Use Term Mode for Simple Proofs

**Before:**
```lean
have hœÉ_1œÅ_nonneg : 0 ‚â§ œÉSq * (1 - œÅ) := by
  apply mul_nonneg hœÉSq_nonneg
  linarith [_hœÅ_bd.2]
```

**After:**
```lean
have hœÉ_1œÅ_nonneg : 0 ‚â§ œÉSq * (1 - œÅ) :=
  mul_nonneg hœÉSq_nonneg (by linarith [_hœÅ_bd.2])
```

**When to use term mode:**
- The proof is a simple function application
- The result is more concise and readable
- You can still use `by` for complex sub-proofs

### Pattern 7: Reuse Common Intermediate Definitions

**Before:**
```lean
have step5 : ... := by
  have hbdd : BddAbove ... := ...
  ...

have hsup_nonneg : 0 ‚â§ ‚®Ü j, |c j| := by
  have hbdd : BddAbove ... := ...  -- duplicate!
  ...
```

**After:**
```lean
have hbdd : BddAbove (Set.range fun j : Fin n => |c j|) := ...

have step5 : ... := by
  ...  -- uses hbdd

have hsup_nonneg : 0 ‚â§ ‚®Ü j, |c j| := by
  ...  -- also uses hbdd
```

**When to factor out:**
- The same definition appears multiple times
- The definition is used in multiple proofs
- Factoring improves clarity of proof dependencies

### Pattern 8: Simplify `simpa` Wrappers

**Before:**
```lean
have h_sq : (‚àë i, c i)^2 = ‚àë i, ‚àë j, c i * c j := by
  simpa [pow_two] using
    (Finset.sum_mul_sum (s := Finset.univ) (t := Finset.univ)
      (f := fun i => c i) (g := fun j => c j))
```

**After:**
```lean
have h_sq : (‚àë i, c i)^2 = ‚àë i, ‚àë j, c i * c j := by
  rw [pow_two, Finset.sum_mul_sum (s := Finset.univ) (t := Finset.univ)
    (f := fun i => c i) (g := fun j => c j)]
```

**When to replace `simpa`:**
- It's just unfolding a definition
- `rw` or `simp only` is clearer
- The `using` clause is doing the real work

### Pattern 9: Remove Commented-Out Code

After the file compiles successfully, remove:
- Failed proof attempts
- Debugging traces
- Commented-out alternative approaches
- Old versions of refactored code

**Exception - Keep brief comments explaining:**
- Why a particular approach was chosen
- Known issues or future TODOs
- Strategies for completing sorries

**Good comment (keep):**
```lean
-- Strategy: Use Dynkin's œÄ-Œª theorem to extend from rectangles
-- TODO: Complete the œÄ-system verification
```

**Bad comment (remove):**
```lean
-- have h1 := ...
-- rw [...]
-- -- this doesn't work
-- have h2 := ...
-- -- also doesn't work
```

### Pattern 10: Factor Large Proofs into Lemmas

When a proof becomes very long (>50 lines) with clear logical sections, factor major steps into separate lemmas:

**Before (in main theorem):**
```lean
theorem main_result : ... := by
  -- 150 lines of proof with steps 1-6
  calc ...
```

**After:**
```lean
lemma step1 : ... := by
  -- 20 lines

lemma step2 : ... := by
  -- 25 lines

theorem main_result : ... := by
  calc ... = ... := step1
       _ = ... := step2
       ...
```

**Benefits:**
- Main theorem structure is immediately clear
- Individual steps are independently testable
- Lemmas can be reused elsewhere
- Easier to review and maintain

## Optimal Workflow (Research-Validated)

Time-tested across 3 sessions, 19 proofs, ~100 lines removed.

### Phase 1: Pattern Discovery (5 minutes)

**Use systematic search, not sequential reading:**

```bash
# 1. Find all let+have+exact patterns (HIGHEST value)
grep -A 10 "let .*:=" MyFile.lean | grep -B 8 "exact"

# 2. Find arithmetic chains
grep -n "calc.*<.*by" MyFile.lean

# 3. Find by-exact patterns
grep -B 1 "exact" MyFile.lean | grep "by$"

# 4. Find constructor branches with multiple lines
grep -A 5 "constructor" MyFile.lean | grep "intro"
```

**Expected output:** 10-15 optimization targets per file

### Phase 2: Candidate Generation (10 minutes)

For each pattern found:

1. **Read proof context** (2 minutes)
   - Understand what the proof does
   - Check type signatures
   - Note any dependencies

2. **Generate 2-3 candidates** (3 minutes)
   - **Candidate A:** Direct inline (most aggressive)
   - **Candidate B:** Partial inline (keep let or have for type inference)
   - **Candidate C:** With automation (if arithmetic)

3. **Test with lean_multi_attempt** (1 minute)
   - All candidates tested in parallel
   - Immediate compilation feedback

4. **Pick shortest that compiles** (30 seconds)
   - Use token counting quick reference
   - Prefer readability if token difference <10%

### Phase 3: Application (5 minutes)

1. Apply winning candidate
2. Verify compilation with `lake build`
3. Move to next pattern
4. Batch related changes together

### Phase 4: Batch Verification (5 minutes)

1. Run `lake build` on all modified files
2. Commit with detailed message documenting patterns optimized
3. Document any new patterns discovered

**Total time:** ~25 minutes per file with 10-15 targets

**Expected results:** 30-40% size reduction, 100% compilation success

### Decision Tree: Should I Optimize This Proof?

```
Is proof ‚â•5 lines?
‚îú‚îÄ No ‚Üí Skip (not worth effort)
‚îî‚îÄ Yes ‚Üí Continue

Does it match let+have+exact pattern?
‚îú‚îÄ Yes ‚Üí HIGH PRIORITY (60-80% reduction likely) ‚≠ê
‚îî‚îÄ No ‚Üí Continue

Is it arithmetic (calc/linarith/omega)?
‚îú‚îÄ Yes ‚Üí MEDIUM PRIORITY (30-50% reduction likely)
‚îî‚îÄ No ‚Üí Continue

Does it have multiple inline haves?
‚îú‚îÄ Yes ‚Üí LOW PRIORITY (10-30% reduction)
‚îî‚îÄ No ‚Üí Skip (minimal benefit)
```

### Decision Tree: Which Candidate Should I Try First?

```
Pattern: let x := A; have h : P x := B; exact C x h

Candidate priority:
1. Full inline: exact C (fun ... => A) (fun ... => B)
   ‚Ä¢ Shortest when types are simple
   ‚Ä¢ Try first with lean_multi_attempt

2. Partial inline: let x := A; exact C x B
   ‚Ä¢ Middle ground for type inference
   ‚Ä¢ Fallback if #1 fails

3. Keep structure: let x := A; have h := B; exact C x h
   ‚Ä¢ Minimal optimization
   ‚Ä¢ Last resort if others fail
```

## Systematic Workflow (Detailed Patterns)

### Pass 1: Structural Cleanup

1. Remove commented-out code
2. Factor out major proof blocks into lemmas (if needed)
3. Move definitions closer to their use sites
4. Reorder lemmas into logical groups

### Pass 2: Local Simplifications

5. Inline single-use intermediate lemmas
6. Consolidate simple rewrites
7. Replace `simpa` with direct `rw` or `simp only` where clearer
8. Remove unnecessary `by exact` wrappers
9. Convert simple tactic proofs to term mode

### Pass 3: Proof Chain Improvements

10. Merge sequential simp/rw steps
11. Replace `trans` + `calc` with single `calc`
12. Eliminate nested helpers in calc chains
13. Identify and reuse common sub-expressions

### Pass 4: Verification

14. Run `lake build` to ensure everything still compiles
15. Run `#lint` to check for new issues
16. Read through simplified proofs to verify readability

## Success Metrics

Good simplifications should:
- **Reduce line count** without sacrificing clarity
- **Improve readability** by removing redundant steps
- **Maintain proof structure** - mathematical logic should still be clear
- **Follow mathlib conventions** - align with standard proof patterns

## Anti-Patterns to Avoid

### Don't Use Semicolons Just to Combine Lines

**Bad (no real savings):**
```lean
-- Before (2 lines, 6 tokens): intro x \n exact proof
intro x
exact proof

-- After (1 line, 6 tokens): intro x ; exact proof
intro x; exact proof  -- Semicolon is a token! No savings!
```

**Why this is wrong:** Semicolons are tokens. Only combine with semicolons when:
1. You're also inlining/eliminating statements (actual token reduction)
2. The operations are sequential and natural (readability maintained)
3. You save ‚â•2 lines

**Good use of semicolons (sequential operations with savings):**
```lean
-- ‚úÖ Example 1: Inline + semicolon (saves tokens)
-- Before (multiple statements): ~15 tokens
constructor
¬∑ intro k hk
  exact hX m k hk

-- After (inline + semicolon): ~10 tokens
constructor
¬∑ intro k hk; exact hX m k hk

-- ‚úÖ Example 2: Sequential proof steps (natural flow)
-- Before (6 lines)
ext x
simp only [Finset.mem_image, Finset.mem_univ, true_and, iff_true]
use œÉ.symm x
simp

-- After (1 line, natural sequence)
ext x; simp only [Finset.mem_image, Finset.mem_univ, true_and, iff_true]; use œÉ.symm x; simp
```

**When semicolons ARE worth it:**
- ‚úÖ Sequential operations (ext ‚Üí simp ‚Üí use ‚Üí simp)
- ‚úÖ Saves ‚â•2 lines with maintained readability
- ‚úÖ Simple steps (intro, ext, use, simp, etc.)

**When semicolons are NOT worth it:**
- ‚ùå Just combining 2 lines (no token savings)
- ‚ùå Creates unreadable long lines (>100 chars)
- ‚ùå Complex steps needing inspection

### Don't Over-Automate Simple Lemmas

**Bad:**
```lean
-- Attempted automation (fails + more setup): ~8 tokens
exact hX m (fun i => k + i.val) (fun i j hij => by omega)
-- ‚ùå Doesn't work (counterexample error)!

-- Direct lemma (works + shorter): ~6 tokens
exact hX m (fun i => k + i.val) (fun i j hij => Nat.add_lt_add_left hij k)
-- ‚úÖ Works perfectly
```

**Rule:** If mathlib has a direct lemma ‚â§5 tokens, use it. Don't force automation.

### Don't Inline Complex Pattern Matching

**Bad:**
```lean
-- DON'T try to inline this!
cases x with
| inl ha =>
  have h1 := ...
  have h2 := ...
  exact combine h1 h2
| inr hb =>
  have h3 := ...
  exact other_result h3
```

**Why:** The case structure is semantic, not mechanical. Inlining would destroy readability without significant token savings.

### Don't Over-Inline

**Bad:**
```lean
calc x = y := by
  apply Finset.sum_congr rfl; intro i _; apply Finset.sum_congr rfl; intro j hj;
  have h1 := ...; have h2 := ...; have h3 := ...; simp [h1, h2, h3, mul_comm, mul_assoc];
  rw [h4, h5]; ring; linarith [h6, h7]
```

If inlining creates an unreadable proof, keep intermediate steps.

### Don't Remove Helpful Names

**Bad:**
```lean
have : ... := by
  ... -- 10 lines of proof
have : ... := by
  ... -- uses first anonymous have
```

**Good:**
```lean
have hkey_property : ... := by
  ... -- 10 lines of proof
have hconclusion : ... := by
  ... -- uses hkey_property
```

If an intermediate result has mathematical significance, give it a descriptive name.

### Don't Sacrifice Clarity for Brevity

**Bad:**
```lean
theorem main : ... :=
  ‚ü®Œª h => h.1.2.1, Œª h => ‚ü®‚ü®h.1, h.2‚ü©, ‚ü®h.3, h.4‚ü©, ‚ü®h.5, h.6‚ü©‚ü©‚ü©
```

**Good:**
```lean
theorem main : ... := by
  constructor
  ¬∑ intro h; exact h.prop1
  ¬∑ intro h; exact ‚ü®h.left, h.right, h.key‚ü©
```

If term-mode proof becomes cryptic, stick with tactic mode.

## Related References

- [mathlib-style.md](mathlib-style.md) - Mathlib style conventions
- [tactics-reference.md](tactics-reference.md) - Tactic catalog
- [domain-patterns.md](domain-patterns.md) - Domain-specific patterns
